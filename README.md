# Procesador de Audio y Video con IA â€” Parra Postpartum v1

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![License](https://img.shields.io/github/license/tu_usuario/tu_repo)
![Platform](https://img.shields.io/badge/platform-linux--mac--win-green)

---

## ğŸš€ DescripciÃ³n

**Parra Postpartum v1** es un pipeline automatizado para el procesamiento, diarizaciÃ³n y transcripciÃ³n de archivos de audio y video, pensado para anÃ¡lisis clÃ­nico, entrevistas, investigaciÃ³n en salud, lingÃ¼Ã­stica y mucho mÃ¡s.

**Incluye:**
- ConversiÃ³n automÃ¡tica de video a audio (MP3)
- Preprocesamiento y ecualizaciÃ³n de voz humana para mejorar la calidad de la transcripciÃ³n
- DiarizaciÃ³n automÃ¡tica (identificaciÃ³n de hablantes) usando [pyannote.audio](https://github.com/pyannote/pyannote-audio)
- TranscripciÃ³n segmentada por hablante usando un modelo Whisper fine-tuneado

---

## ğŸ§  Â¿CÃ³mo funciona el pipeline?

El programa sigue este **flujo automÃ¡tico** para cualquier archivo de audio o video que proceses:

1. **ConversiÃ³n de video a audio:** Si el archivo es video, lo convierte automÃ¡ticamente a MP3 usando ffmpeg, sin que tengas que hacer nada extra.
2. **Preprocesamiento y ecualizaciÃ³n:** Aplica filtrado y normalizaciÃ³n al audio, enfocÃ¡ndose en la banda de la voz humana (300â€“3400 Hz) para mÃ¡xima claridad.
3. **DiarizaciÃ³n automÃ¡tica de hablantes:** Con modelos deep learning (pyannote.audio), identifica cuÃ¡ndo y quiÃ©n estÃ¡ hablando en cada segmento.
4. **TranscripciÃ³n segmentada:** Utiliza un modelo Whisper fine-tuneado para transcribir cada segmento, alineando texto, tiempo y hablante.
5. **OrganizaciÃ³n y guardado de resultados:** Todos los outputs (audio procesado, segmentos, transcripciones alineadas) se guardan ordenadamente en una carpeta dedicada a cada archivo de entrada.

**Todo esto ocurre de forma automÃ¡tica con una sola lÃ­nea de comando.**

---

## ğŸ“¦ Requisitos

- **Python 3.10+**  
- Sistema operativo: Windows, MacOS, o Linux
- [ffmpeg](https://ffmpeg.org/) instalado (para conversiÃ³n de video a audio)
- Un [token de HuggingFace](https://huggingface.co/settings/tokens) vÃ¡lido (para descargar modelos pyannote/Whisper)
- GPU NVIDIA (opcional, recomendado para grandes volÃºmenes)

---

## ğŸ› ï¸ InstalaciÃ³n paso a paso

1. **Clona el repositorio**

   ```sh
   git clone https://github.com/tu_usuario/tu_repo.git
   cd tu_repo

---

## ğŸ› ï¸ InstalaciÃ³n rÃ¡pida

```sh
git clone https://github.com/tu_usuario/tu_repo.git
cd tu_repo
python3 -m pip install --upgrade pip
python3 scripts/setup_env.py

---

Configura tu archivo de entorno
Copia el ejemplo y pega tu token de HuggingFace:

sh
cp .env.example .env
Edita el archivo .env para incluir tu token:

ini
HUGGINGFACE_TOKEN=tu_token_de_huggingface_aqui
(Opcional) Instala ffmpeg si no lo tienes
Ubuntu/Debian: sudo apt install ffmpeg

MacOS: brew install ffmpeg

Windows: Descarga desde ffmpeg.org

âš¡ Uso del programa
Activar entorno virtual
En Mac/Linux:

sh
source Parra-Postpartum-v1/bin/activate
En Windows:

sh
Parra-Postpartum-v1\Scripts\activate
Procesar un archivo (audio o video)
sh
python post_partum.py <archivo_audio_o_video>
Ejemplo de uso:

sh
python post_partum.py entrevista.mp3
python post_partum.py reunion_familiar.mp4
El programa detecta automÃ¡ticamente el tipo de archivo y realiza todas las etapas de conversiÃ³n, procesamiento, diarizaciÃ³n y transcripciÃ³n, guardando todo en una carpeta `/resultados/<nombre_archivo>/.

ğŸ­ Funcionamiento detallado del script principal (post_partum.py)
Detecta el tipo de archivo:

Si es audio soportado (.wav, .mp3, .m4a), lo procesa directo.

Si es video (.mp4, .avi, .mov, etc.), extrae el audio a MP3 automÃ¡ticamente.

Preprocesa el audio:

Convierte a WAV, fuerza 16kHz mono.

Aplica filtrado pasa banda (300â€“3400 Hz) para mejorar calidad.

Normaliza el volumen para evitar clipping.

Diariza:

Usa pyannote para detectar intervalos y asignar un label a cada hablante.

Guarda un JSON tipo:

json
[
  {"start_time": 0.0, "end_time": 11.5, "speaker": "SPEAKER_00"},
  {"start_time": 11.5, "end_time": 22.7, "speaker": "SPEAKER_01"}
]
Transcribe:

Toma cada segmento identificado y lo transcribe usando Whisper.

Asigna la transcripciÃ³n a cada segmento, ejemplo:

json
[
  {"start_time": 0.0, "end_time": 11.5, "speaker": "SPEAKER_00", "transcript": "buenos dÃ­as a todos..."},
  {"start_time": 11.5, "end_time": 22.7, "speaker": "SPEAKER_01", "transcript": "gracias por venir."}
]
Guarda resultados en /resultados/<nombre_archivo>/:

Audio original

Audio convertido (si aplica)

Audio procesado (*_processed.wav)

diarization_results.json

aligned_transcription.json

ğŸ“‚ Estructura del proyecto
text
audio-ia-pipeline/
â”‚
â”œâ”€â”€ funciones/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ procesamiento_de_audio.py
â”‚   â”œâ”€â”€ diarizacion.py
â”‚   â”œâ”€â”€ transcripcion.py
â”‚   â””â”€â”€ convertir_video_a_audio.py
â”‚
â”œâ”€â”€ resultados/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ejemplo_proceso.ipynb
â”œâ”€â”€ tests/
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup_env.py
â”œâ”€â”€ post_partum.py
â””â”€â”€ LICENSE
ğŸ“ Ejemplo de salida de resultados
diarization_results.json

json
[
  {"start_time": 0.0, "end_time": 9.8, "speaker": "SPEAKER_00"},
  {"start_time": 9.8, "end_time": 16.5, "speaker": "SPEAKER_01"}
]
aligned_transcription.json

json
[
  {"start_time": 0.0, "end_time": 9.8, "speaker": "SPEAKER_00", "transcript": "buenos dÃ­as, doctor..."},
  {"start_time": 9.8, "end_time": 16.5, "speaker": "SPEAKER_01", "transcript": "buenos dÃ­as, cÃ³mo estÃ¡."}
]
ğŸ› ï¸ Troubleshooting
No se activa el entorno virtual:
AsegÃºrate de usar python3.10 o superior y de estar en la carpeta raÃ­z del proyecto.

Error "No module named X":
Verifica que el entorno virtual estÃ¡ activado antes de correr el script.

Problemas con torch/cu121 en Mac o sin GPU:
Cambia las lÃ­neas de torch en requirements.txt por versiones sin +cu121 ni el index extra y corre:

sh
pip install torch torchvision torchaudio
ffmpeg no encontrado:
InstÃ¡lalo segÃºn tu sistema (ver secciÃ³n de requisitos).

Token de HuggingFace invÃ¡lido:
Genera un nuevo token desde huggingface.co/settings/tokens y colÃ³calo en tu .env.

ğŸ”‘ Variables de entorno
Copia .env.example a .env y coloca tu token de HuggingFace:

sh
cp .env.example .env
Luego edita el archivo .env:

ini
HUGGINGFACE_TOKEN=tu_token_de_huggingface_aqui
Este token es necesario para descargar modelos privados de pyannote/Whisper.

ğŸ™‹â€â™‚ï¸ Â¿CÃ³mo contribuir?
Haz un fork del repositorio y clona tu copia.

Crea una rama nueva para tus cambios:

sh
git checkout -b mi-nueva-funcionalidad
Haz tus mejoras y comenta el cÃ³digo.

Haz un commit claro y push:

sh
git add .
git commit -m "Agregada nueva funcionalidad de X"
git push origin mi-nueva-funcionalidad
Haz un Pull Request desde tu fork.
Â¡Tus sugerencias, mejoras y fixes son bienvenidos!

ğŸ“š CrÃ©ditos y referencias
pyannote.audio

Whisper (OpenAI)

Transformers (Hugging Face)

ffmpeg

ğŸªª Licencia
MIT â€” puedes usar y modificar este proyecto libremente.