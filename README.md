# Procesador de Audio y Video con IA ‚Äî Parra Postpartum v1

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![License](https://img.shields.io/github/license/tu_usuario/tu_repo)
![Platform](https://img.shields.io/badge/platform-linux--mac--win-green)

---

## üöÄ Descripci√≥n

**Parra Postpartum v1** es un pipeline automatizado para el procesamiento, diarizaci√≥n y transcripci√≥n de archivos de audio y video, pensado para an√°lisis cl√≠nico, entrevistas, investigaci√≥n en salud, ling√º√≠stica y mucho m√°s.

**Incluye:**
- Conversi√≥n autom√°tica de video a audio (MP3)
- Preprocesamiento y ecualizaci√≥n de voz humana para mejorar la calidad de la transcripci√≥n
- Diarizaci√≥n autom√°tica (identificaci√≥n de hablantes) usando [pyannote.audio](https://github.com/pyannote/pyannote-audio)
- Transcripci√≥n segmentada por hablante usando un modelo Whisper fine-tuneado

---

## üß† ¬øC√≥mo funciona el pipeline?

El programa sigue este **flujo autom√°tico** para cualquier archivo de audio o video que proceses:

1. **Conversi√≥n de video a audio:** Si el archivo es video, lo convierte autom√°ticamente a MP3 usando ffmpeg.
2. **Preprocesamiento y ecualizaci√≥n:** Filtra y normaliza el audio, enfoc√°ndose en la banda de la voz humana (300‚Äì3400 Hz) para m√°xima claridad.
3. **Diarizaci√≥n autom√°tica de hablantes:** Con modelos deep learning (pyannote.audio), identifica cu√°ndo y qui√©n est√° hablando en cada segmento.
4. **Transcripci√≥n segmentada:** Utiliza un modelo Whisper fine-tuneado para transcribir cada segmento, alineando texto, tiempo y hablante.
5. **Organizaci√≥n y guardado de resultados:** Todos los outputs (audio procesado, segmentos, transcripciones alineadas) se guardan ordenadamente en una carpeta dedicada a cada archivo de entrada.

**Todo esto ocurre de forma autom√°tica con una sola l√≠nea de comando.**

---

## üì¶ Requisitos

- **Python 3.10+**
- Sistema operativo: Windows, MacOS, o Linux
- [ffmpeg](https://ffmpeg.org/) instalado (para conversi√≥n de video a audio)
- Un [token de HuggingFace](https://huggingface.co/settings/tokens) v√°lido (para descargar modelos pyannote/Whisper)
- GPU NVIDIA (opcional, recomendado para grandes vol√∫menes)

---

## üõ†Ô∏è Instalaci√≥n paso a paso

1. **Clona el repositorio**
   ```sh
   git clone https://github.com/tu_usuario/tu_repo.git
   cd tu_repo


2. **Instala las dependencias en un entorno virtual**

   ```sh
   python3 -m pip install --upgrade pip
   python3 scripts/setup_env.py
   ```

   > Esto instalar√° todas las dependencias en un entorno virtual llamado **Parra-Postpartum-v1**.

3. **Copia el ejemplo y pega tu token de HuggingFace:**

   ```sh
   cp .env.example .env
   ```

   Edita el archivo `.env` para incluir tu token:

   ```ini
   HUGGINGFACE_TOKEN=tu_token_de_huggingface_aqui
   ```

4. **(Opcional) Instala ffmpeg si no lo tienes**

   * **Ubuntu/Debian:**

     ```sh
     sudo apt install ffmpeg
     ```
   * **MacOS:**

     ```sh
     brew install ffmpeg
     ```
   * **Windows:**
     Descarga desde [ffmpeg.org](https://ffmpeg.org/)

---

## ‚ö° Uso del programa

### Activar entorno virtual

* **En Mac/Linux:**

  ```sh
  source Parra-Postpartum-v1/bin/activate
  ```
* **En Windows:**

  ```sh
  Parra-Postpartum-v1\Scripts\activate
  ```

### Procesar un archivo (audio o video)

```sh
python post_partum.py <archivo_audio_o_video>
```

**Ejemplo de uso:**

```sh
python post_partum.py entrevista.mp3
python post_partum.py reunion_familiar.mp4
```

El programa detecta autom√°ticamente el tipo de archivo y realiza todas las etapas de conversi√≥n, procesamiento, diarizaci√≥n y transcripci√≥n, guardando todo en una carpeta `/resultados/<nombre_archivo>/`.

---

## üè≠ Funcionamiento detallado del script principal (`post_partum.py`)

1. **Detecta el tipo de archivo:**

   * Si es audio soportado (`.wav`, `.mp3`, `.m4a`), lo procesa directo.
   * Si es video (`.mp4`, `.avi`, `.mov`, etc.), extrae el audio a MP3 autom√°ticamente.

2. **Preprocesa el audio:**

   * Convierte a WAV, fuerza 16kHz mono.
   * Aplica filtrado pasa banda (300‚Äì3400 Hz) para mejorar calidad.
   * Normaliza el volumen para evitar clipping.

3. **Diariza:**

   * Usa pyannote para detectar intervalos y asignar un label a cada hablante.
   * Guarda un JSON tipo:

     ```json
     [
       {"start_time": 0.0, "end_time": 11.5, "speaker": "SPEAKER_00"},
       {"start_time": 11.5, "end_time": 22.7, "speaker": "SPEAKER_01"}
     ]
     ```

4. **Transcribe:**

   * Toma cada segmento identificado y lo transcribe usando Whisper.
   * Asigna la transcripci√≥n a cada segmento, ejemplo:

     ```json
     [
       {"start_time": 0.0, "end_time": 11.5, "speaker": "SPEAKER_00", "transcript": "buenos d√≠as a todos..."},
       {"start_time": 11.5, "end_time": 22.7, "speaker": "SPEAKER_01", "transcript": "gracias por venir."}
     ]
     ```

5. **Guarda resultados en `/resultados/<nombre_archivo>/`:**

   * Audio original
   * Audio convertido (si aplica)
   * Audio procesado (`*_processed.wav`)
   * `diarization_results.json`
   * `aligned_transcription.json`

---

## üìù Ejemplo de salida de resultados

* **diarization\_results.json**

  ```json
  [
    {"start_time": 0.0, "end_time": 9.8, "speaker": "SPEAKER_00"},
    {"start_time": 9.8, "end_time": 16.5, "speaker": "SPEAKER_01"}
  ]
  ```
* **aligned\_transcription.json**

  ```json
  [
    {"start_time": 0.0, "end_time": 9.8, "speaker": "SPEAKER_00", "transcript": "buenos d√≠as, doctor..."},
    {"start_time": 9.8, "end_time": 16.5, "speaker": "SPEAKER_01", "transcript": "buenos d√≠as, c√≥mo est√°."}
  ]
  ```

---

## üõ†Ô∏è Troubleshooting

* **No se activa el entorno virtual:**
  Aseg√∫rate de usar `python3.10` o superior y de estar en la carpeta ra√≠z del proyecto.

* **Error ‚ÄúNo module named X‚Äù:**
  Verifica que el entorno virtual est√° activado antes de correr el script.

* **Problemas con torch/cu121 en Mac o sin GPU:**
  Cambia las l√≠neas de torch en `requirements.txt` por versiones sin `+cu121` ni el index extra y corre:

  ```sh
  pip install torch torchvision torchaudio
  ```

* **ffmpeg no encontrado:**
  Inst√°lalo seg√∫n tu sistema (ver secci√≥n de requisitos).

* **Token de HuggingFace inv√°lido:**
  Genera un nuevo token desde [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) y col√≥calo en tu `.env`.

---

## üîë Variables de entorno

Copia `.env.example` a `.env` y coloca tu token de HuggingFace:

```sh
cp .env.example .env
```

Luego edita el archivo `.env`:

```ini
HUGGINGFACE_TOKEN=tu_token_de_huggingface_aqui
```

Este token es necesario para descargar modelos privados de pyannote/Whisper.

---

## üôã‚Äç‚ôÇÔ∏è ¬øC√≥mo contribuir?

1. Haz un fork del repositorio y clona tu copia.
2. Crea una rama nueva para tus cambios:

   ```sh
   git checkout -b mi-nueva-funcionalidad
   ```
3. Haz tus mejoras y comenta el c√≥digo.
4. Haz un commit claro y push:

   ```sh
   git add .
   git commit -m "Agregada nueva funcionalidad de X"
   git push origin mi-nueva-funcionalidad
   ```
5. Haz un Pull Request desde tu fork.
   ¬°Tus sugerencias, mejoras y fixes son bienvenidos!

---

¬°Por supuesto! Aqu√≠ tienes una **descripci√≥n de uso** profesional y clara, lista para README, que incluye:

* Versi√≥n de Python requerida
* Especificaci√≥n de CUDA
* Recomendaci√≥n de GPU
* Nota de uso de Vast.ai
* Indicaci√≥n de que el ambiente virtual de este repo ya permite hacer fine-tuning

---

## üß© Descripci√≥n del Finetuning realizado para Whisper v3

Este script permite realizar **fine-tuning de modelos Whisper para transcripci√≥n de voz en espa√±ol**, integrando todo el pipeline: descarga/preprocesamiento del dataset, aumentaci√≥n de datos, extracci√≥n de features y entrenamiento supervisado, todo en un entorno controlado y reproducible.

* **Python requerido:** 3.10
* **CUDA recomendado:** CUDA 12.1+ (se ha probado exitosamente con PyTorch 2.3.0/cu121)
* **Ambiente:** El entorno virtual definido en este repositorio (`Parra-Postpartum-v1`) incluye todas las dependencias necesarias para ejecutar el preprocesamiento y entrenamiento, tanto en CPU como GPU.
* **Plataforma usada:** Este fine-tuning se realiz√≥ originalmente en un servidor cloud de **Vast.ai**, lo que garantiza compatibilidad y rendimiento √≥ptimo en entornos con GPU dedicadas (por ejemplo, NVIDIA RTX 3090/4090, A100, V100, etc.).
* **Entrenamiento en CPU es posible**, pero muy lento y no recomendado para datasets medianos/grandes.

### **¬øC√≥mo usarlo?**

1. **Activa el ambiente virtual del repositorio:**

   * En Mac/Linux:

     ```sh
     source Parra-Postpartum-v1/bin/activate
     ```
   * En Windows:

     ```sh
     Parra-Postpartum-v1\Scripts\activate
     ```

2. **Aseg√∫rate de tener CUDA 12.1+ instalado y drivers NVIDIA actualizados** si vas a entrenar en GPU.
   Puedes revisar la instalaci√≥n con:

   ```sh
   nvcc --version
   ```

3. **Ejecuta el script:**

   ```sh
   python scripts/finetuning-Whisper-v3.py
   ```

   El script se encargar√° de descargar/preprocesar el dataset, realizar aumentaci√≥n de audio, preparar features y entrenar el modelo Whisper. Los modelos y procesadores fine-tuneados se guardan autom√°ticamente en la carpeta `./whisper-finetuned-augment`.

4. **Customizaci√≥n:**
   Si deseas usar otro dataset o modelo base, edita las rutas/identificadores dentro del script antes de ejecutarlo.

### **Requisitos computacionales**

| Recurso          | M√≠nimo                             | Ideal/Recomendado                         |
| ---------------- | ---------------------------------- | ----------------------------------------- |
| **Python**       | 3.10                               | 3.10 (mismo entorno que el repo)          |
| **CUDA**         | No necesario (pero lento en CPU)   | CUDA 12.1+ (NVIDIA, compatible PyTorch)   |
| **GPU**          | Opcional (CPU posible, pero lento) | NVIDIA 3090/4090, A100, V100, 16 GB+ VRAM |
| **RAM**          | 16 GB                              | 32 GB+                                    |
| **Disco**        | 20 GB libres                       | 50 GB+ si usas datasets grandes           |
| **SO**           | Ubuntu 20.04+, MacOS, Windows 10+  | Ubuntu 22.04 LTS o similar                |
| **Dependencias** | Incluidas en el ambiente virtual   | Incluidas en el ambiente virtual          |

**Notas:**

* Si tienes una GPU NVIDIA y los drivers/CUDA instalados correctamente, el entrenamiento aprovechar√° autom√°ticamente la aceleraci√≥n por hardware.
* Se recomienda entrenar en plataformas tipo **Vast.ai**, Paperspace o Google Colab Pro+ para acelerar el proceso.
* El entorno virtual proporcionado en este repositorio te permite no solo inferencia sino tambi√©n **entrenar y hacer fine-tuning de modelos Whisper** sin pasos adicionales.


---

## üìö Cr√©ditos y referencias

* [pyannote.audio](https://github.com/pyannote/pyannote-audio)
* [Whisper (OpenAI)](https://github.com/openai/whisper)
* [Transformers (Hugging Face)](https://huggingface.co/docs/transformers/index)
* [ffmpeg](https://ffmpeg.org/)

---

¬°Perfecto! Aqu√≠ tienes la **cita formal** para el dataset **CIEMPIESS TEST** y un bloque listo para incluir en tu README, documentaci√≥n, art√≠culo o secci√≥n de agradecimientos.
Incluyo tambi√©n un p√°rrafo de reconocimiento y la referencia en formato BibTeX.

---

## üìÇ Dataset utilizado: CIEMPIESS TEST

Este proyecto utiliza el **CIEMPIESS TEST CORPUS** para la evaluaci√≥n y prueba de modelos de reconocimiento autom√°tico de voz en espa√±ol mexicano.

> **CIEMPIESS TEST** es un corpus equilibrado por g√©nero, compuesto por m√°s de 3,500 grabaciones de voz espont√°nea en espa√±ol mexicano, recopiladas y transcritas por el programa de servicio social "Desarrollo de Tecnolog√≠as del Habla" de la Facultad de Ingenier√≠a de la UNAM.
> Las grabaciones provienen de "RADIO-IUS", estaci√≥n de la Facultad de Derecho de la UNAM, y el corpus fue cuidadosamente anotado y verificado para asegurar su calidad.
> Se distribuye bajo la licencia [CC-BY-SA-4.0](https://creativecommons.org/licenses/by-sa/4.0/) y est√° disponible en [Hugging Face Datasets](https://huggingface.co/datasets/ciempiess/ciempiess_test) y en [LDC2019S07](https://catalog.ldc.upenn.edu/LDC2019S07).

**Referencia:**

```bibtex
@misc{carlosmenaciempiesstest2019,
  title={CIEMPIESS TEST CORPUS: Audio and Transcripts of Mexican Spanish Broadcast Conversations.},
  ldc_catalog_no={LDC2019S07},
  DOI={https://doi.org/10.35111/xdx5-n815},
  author={Hernandez Mena, Carlos Daniel},
  journal={Linguistic Data Consortium, Philadelphia},
  year={2019},
  url={https://catalog.ldc.upenn.edu/LDC2019S07},
}
```

**C√≥mo citar este recurso en tu trabajo:**

> Hernandez Mena, Carlos Daniel. *CIEMPIESS TEST CORPUS: Audio and Transcripts of Mexican Spanish Broadcast Conversations.* Linguistic Data Consortium, Philadelphia, 2019. DOI: [10.35111/xdx5-n815](https://doi.org/10.35111/xdx5-n815).

**Reconocimientos especiales:**

* Al programa de servicio social "Desarrollo de Tecnolog√≠as del Habla" de la Facultad de Ingenier√≠a, UNAM.
* A la Lic. C√©sar Gabriel Alanis Merchand y el Mtro. Ricardo Rojas Ar√©valo de la Facultad de Derecho, UNAM, por la donaci√≥n de grabaciones.
* A M√≥nica Alejandra Ruiz L√≥pez por la verificaci√≥n de las transcripciones.


---

## ü™™ Licencia

MIT ‚Äî puedes usar y modificar este proyecto libremente.



---

